{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importer ipysheet, dette bruker vi for å vise dataframe som ett spreadsheet\n",
    "import ipysheet\n",
    "\n",
    "# Standardimporter\n",
    "import dapla as dp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78723d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eksempeldataframe\n",
    "diam = dp.read_pandas(\"/felles/kurs/helt_python/diamonds\")\n",
    "# Til dette eksempelet bruker vi de første ti radene\n",
    "diam = diam.head(10)\n",
    "# Det er viktig at du begrenser dette noe, det er ikke å anbefale å sitte å mikroeditere flere hundre rader om gangen.\n",
    "# Hverken for din egen del, eller for jupyters prosessering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e103b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log path (du må ha skriverettigheter)\n",
    "microlog_path = '/felles/trash/diamonds_mikroeditlog'\n",
    "# Dette vil da etter hvert bli en \"samling\" av log-entries. Inkludert bruker og tidspunkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2fdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag ett interaktivt regneark med ipysheets\n",
    "sheet = ipysheet.from_dataframe(diam)\n",
    "# Display regnearket \n",
    "sheet\n",
    "# Under kan du da redigere dataen direkte i cellene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12a28c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kjør denne cellen, og videre nedover, etter at du er ferdig med å redigere.\n",
    "diam_edit = ipysheet.to_dataframe(sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1142064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne er inkorporert i funksjonen lengre ned, men om du ikke har tenkt å bruke den så må du bruke denne linjen.\n",
    "#diam_edit.index = diam_edit.index.astype(diam.index.dtype)\n",
    "# Det er fordi ipysheet av en eller annen grunn konverterer rad-navnene til \"object\", mens de ser ut til å komme fra dapla som \"int\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f730054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redigeringene bør logges, slik at vi vet hva som ble redigert i denne runden\n",
    "#microlog_now = diam.compare(diam_edit)\n",
    "# Dette vil gi deg det som bør logges, dvs. forskjellen mellom utgangspunktet og den redigerte dataframen.\n",
    "# Under ligger en funksjon, som eksempel på hvordan en slik logg kan tas vare på"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display loggen generert over\n",
    "#microlog_now\n",
    "\n",
    "# Her vil den du starter comparen på være \"self\", diam.compare()\n",
    "# Mens \"other\" vil være den i parantesen fra compare, .compare(diam_other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a5fd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne funksjonen tar to dataframes, men vi forventer at den første er utgangspunktet, og den andre er endret etter mikroeditering\n",
    "# Path er da pathen hvor alle føringene av mikrologgen skal slås sammen.\n",
    "def microlog_write(df1, df2, path):\n",
    "    # Siden denne funksjonen i teorien kan kopieres inn dit man ønsker, så forsikrer vi oss bare om at vi har de importene vi trenger.\n",
    "    from datetime import datetime \n",
    "    import os\n",
    "    from IPython.core.display import display\n",
    "    import dapla as dp\n",
    "    \n",
    "    # Sett index datatype på df2 til å være samme som df1, ipysheet konverterer fra int til object av en eller annen grunn.\n",
    "    df2.index = df2.index.astype(df1.index.dtype)\n",
    "    # Legg gjerne til flere kompensasjoner her, om du oppdager noe comparen lengre ned ikke takler med dine datasett\n",
    "    \n",
    "    # Variablene vi skal skrive til loggen\n",
    "    # Nå-tidspunkt\n",
    "    now = f'{datetime.now():%Y-%m-%d -%H:-%M}'\n",
    "    # Brukernavn i jupyter\n",
    "    user = os.getenv('JUPYTERHUB_USER')\n",
    "    # Her gjør vi selve comparen som finner forskjellen mellom dataframene.\n",
    "    microlog_now = df1.compare(df2)\n",
    "    \n",
    "    \n",
    "    # Det som skal legges på hovedloggen.\n",
    "    # Vi encoder her en hel dataframe som json, og setter den i en celle i en ny dataframe.\n",
    "    microlog_add = pd.DataFrame([[now, user, microlog_now.to_json()]])\n",
    "    # Dvs. om endringene (compare-dataframe) er massive nok, så kan vi få ett issue her at stringen blir for lang...\n",
    "    # Navngi kolonnene, siden de nå er tomme i utgangpunktet (kanskje unødvendig, men why not)\n",
    "    microlog_add.columns = ['Date', 'User', 'Json-dataframe']\n",
    "    \n",
    "    \n",
    "    # Ikke sikkert datasettet finnes...\n",
    "    try:\n",
    "        # Prøv å les inn hovedlagring\n",
    "        microlog_global = dp.read_pandas(path)\n",
    "        # Kombiner det som ligger der, med den nye entryen\n",
    "        microlog_global = pd.concat([microlog_global, microlog_add])\n",
    "        # Skriv den kombinerte totale loggen tilbake til daplalagringen\n",
    "        # Verdivurderingsscorenivå:    .option(\"valuation\", \"XXXX\") = SENSITVE, SHIELDED, INTERNAL eller OPEN\n",
    "        # Dataferdigstillingssnivå:    .option(\"state\", \"XXXX\") = RAW, INPUT, PROCESSED, OUTPUT, PRODUCT, TEMP eller OTHER\n",
    "        dp.write_pandas(microlog_global, path, valuation='INTERNAL', state= 'PROCESSED')\n",
    "        # Print ut dataframen som skrives til loggen nå\n",
    "        display(microlog_now)\n",
    "    # Om den totale loggen ikke er initialisert, så si fra om det.\n",
    "    except Exception as e:\n",
    "        print(f'''Kanskje det ikke finnes noe datasett på denne plasseringen enda?\n",
    "Dobbeltsjekk, så skriv ett tomt datasett til plasseringen:\n",
    "dp.write_pandas(pd.DataFrame(columns = [\"Date\", \"User\", \"Json-dataframe\"]), \"{path}\", valuation=\"INTERNAL\", state= \"OTHER\")\n",
    "''')\n",
    "        # Kan være andre errorer her vi ikke har forutsett, så skriv ut errorbeskjeden også\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her kaller vi write-funksjonen over, med eksempeldataen\n",
    "microlog_write(diam, diam_edit, microlog_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB FARLIG - overskriver / \"tømmer\" loggen, evt. opprett den første gang du \n",
    "#dp.write_pandas(pd.DataFrame(columns = [\"Date\", \"User\", \"Json-dataframe\"]), microlog_path, valuation=\"INTERNAL\", state= \"OTHER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2004d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funksjon for å se på de n-antall siste entryene i den totale loggen.\n",
    "# Num er her antall log-rader du vil hente, default er 1\n",
    "def microlog_print(path, num = 1):\n",
    "    # Siden denne funksjonen i teorien kan kopieres inn dit man ønsker, så forsikrer vi oss bare om at vi har de importene vi trenger.\n",
    "    import dapla as dp\n",
    "    from IPython.core.display import display, HTML\n",
    "    \n",
    "    # Hent fra daplalagring, og begrens return.\n",
    "    micro_sample = dp.read_pandas(path).tail(num)\n",
    "    \n",
    "    # Loop gjennom entryene vi henter (default 1)\n",
    "    for index, row in micro_sample.iterrows():\n",
    "        # Print ut brukernavn og tidspunkt\n",
    "        display(HTML(f'Redigert av {row[\"User\"]}\\nTidspunkt: {row[\"Date\"]}\\n'))\n",
    "        # Konverter dataframen ut av en celle som json, tilbake til en dataframe\n",
    "        display(pd.read_json(row['Json-dataframe']))\n",
    "        # to_json() og read_json() støtter ikke multiindexing i utgangspunktet, så resultatet kan se litt annerledes ut enn ut fra mikrolog_write()-display\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4376b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Her kaller vi print-funksjonen over på pathen vi lagrer hoveddatframen til\n",
    "microlog_print(microlog_path, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ed2877",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
